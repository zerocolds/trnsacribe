# Dockerfile.gpu: CUDA + vLLM + Ollama + Python deps
FROM nvidia/cuda:12.2.0-base-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive

# System deps
RUN apt-get update && apt-get install -y git python3 python3-pip ffmpeg curl && rm -rf /var/lib/apt/lists/*

# vLLM (pip) and other Python deps
RUN pip3 install --upgrade pip && pip3 install vllm requests srt pyannote.audio torch

# Ollama (optional, for local CPU LLM)
RUN curl -fsSL https://ollama.com/install.sh | sh || true

# Copy project
WORKDIR /workspace
COPY . .

# Expose vLLM HTTP port (default 8000)
EXPOSE 8000

# Entrypoint: by default, just bash (user can override)
CMD ["/bin/bash"]
